# 数据集：uci letter-recognition
[uci letter recognition](https://archive.ics.uci.edu/ml/datasets/Letter+Recognition)

# 参数估计方法与非参数估计方法
* 参数估计方法：先假设数据服从某一分布，给出这个分布的大体框架（例如高斯分布），然后求框架中的细微参数（例如高斯分布中的mu和sigma）。
* 非参数估计方法：不做任何假设，一切从数据出发。

参数估计方法会转化为优化问题，可以用最小二乘法、凸优化等方法求解，具有较低的时空复杂度。它之所以复杂度较低，原因就是因为引入了人的先验。它的缺点也很明显，人的先验可能是错误的。  
非参数估计方法通常比较暴力，认为大力出奇迹，直接从原始数据出发，时空复杂度较大。因为它不做假设，因此产生的分布可以千变万化、对数据适应能力强，真正做到不变应万变。 
参数估计是学院派、是"有"，非参数估计是江湖派、是"无"。

给定一个模型，判断它是参数估计还是非参数估计，就看它是否有具体的模型。神经网络是非参数估计。

# parsen窗
Parzen窗（Parzen window）又称为核密度估计（kernel density estimation），是概率论中用来估计未知概率密度函数的非参数方法之一。该方法由Emanuel Parzen于1962年在The Annals of Mathematical Statistics 杂志上发表的论文“On Estimation of a Probability Density Function and Mode” 中首次提出。Nadaraya 和 Watson最早把这一方法用于回归法中。Specht把这一方法用于解决模式分类的问题，并且在1990年发表的论文 “Probabilistic neural networks” 中提出了PNN网络的硬件结构。Ruppert和Cline基于数据集密度函数聚类算法提出了修订的核密度估计方法，对Parzen窗做了一些改进。

Parzen窗方法虽然是在上个世纪60年代提出来的，已经过去了45年的时间，看上去是一种很“古老”的技术，但是现在依然有很多基于Parzen窗方法的论文发表。这说明Parzen窗方法的确有很强的生命力和实用价值，虽然它也存在很多缺点。

parsen窗又称为核密度估计，是一种非参数估计方法。  
parsen窗跟KNN其实是一回事，KNN可以理解为parsen窗的变形，KNN能解决的东西parsen窗一定能解决。  

# parsen窗的适用场景
数据既不稀疏也不密集地分布在全部空间，空间中每一个小网格都能够找到数量不多但决定性足够强的样本。  
如果数据分布得过于密集，会增加复杂度，消耗更多的时间。  
如果数据分布得过于稀疏，有些空间会因为缺少近邻而无法决策。  

这个条件其实很难达到，在高维空间中，数据分布总是稀疏的，很难做到让每个小网格都有样本。

# 有参无参的思辨
parsen窗其实是有参的：它需要假设核函数。矩形窗核函数需要设置宽度，高斯核函数需要设置sigma。而这两个参数的选择并不简单。  
真正不做假设的模型是不存在的。  